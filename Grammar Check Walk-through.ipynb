{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How to run grammar check scripts.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNx1yB8xmfejFUjGmb679bN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camilleoconn/QTM350Contadina/blob/master/Grammar%20Check%20Walk-through.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cWNyQ53ttiv"
      },
      "source": [
        "# Counting Grammar Mistakes for Native versus Non-Native Speakers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLefYTj7qQ6c"
      },
      "source": [
        "Proceed to these steps once you have created your translated JSON files for the written texts you would like to compare. These will have either been created by uploading JSON files to an s3 bucket or to your local directory, and then running this command in the shell:\n",
        "\n",
        "```\n",
        "aws translate translate-text \\\n",
        "            --region region \\\n",
        "            --cli-input-json file://translate.json > translated.json\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz7ozjs9f7oA"
      },
      "source": [
        "### A word on naming files\n",
        "\n",
        "For ease, name the .json files for the native speaker with a prefix that distinguishes it from .json files of the non-native (learner) speaker. For example `n_chinese.json` for the native speaker and `l_chinese.json` for the learner speaker.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gij1OOhMp2sJ"
      },
      "source": [
        "### Now to begin \n",
        "We will be using a Python wrapper for the open-source grammar tool [LanguageTool](https://predictivehacks.com/languagetool-grammar-and-spell-checker-in-python/) called [language_tool_python](https://pypi.org/project/language-tool-python/). From their documentation, this library allows you to detect grammar errors and spelling mistakes through a Python script or through a command-line interface.\n",
        "\n",
        "In the shell run this command to install the wrapper in your local directory.\n",
        "\n",
        "```\n",
        "$ pip install language_tool_python\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdN_sbiuqDi5"
      },
      "source": [
        "### S3 buckets to store files\n",
        "\n",
        "Next we make two S3 buckets into which we will copy over the translated .json files from our local directory so that we can compile the data for processing.\n",
        "\n",
        "The first bucket will contain the translated .json files from the native speaker and the second will contain the translated .json files from the non-native speaker (which we will call \"learner\"). Bucket names must be globally unique, so make sure to adjust the code after the double slashes.\n",
        "\n",
        "```\n",
        "$ aws s3 mb s3://trans-native\n",
        "$ aws s3 mb s3://trans-learner\n",
        "```\n",
        "\n",
        "For additional guidance, refer to [this AWS CLI user guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-services-s3-commands.html) for creating buckets. This step can also be done in the console.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpBgYZ740SLB"
      },
      "source": [
        "### Now from your current working directory, we will copy over the native speaker and learner .json files into their repsective buckets.\n",
        "\n",
        "First, for the native speaker:\n",
        "\n",
        "```\n",
        "# Include all .json files with the \"n_*.json\" format to be copied in bucket\n",
        "$ aws s3 cp $(pwd) s3://trans-native/ --recursive --exclude \"*\" --include \"n_*.json\"\n",
        "```\n",
        "\n",
        "Next, for the learner speaker:\n",
        "\n",
        "```\n",
        "# Include all .json files with the \"l_*.json\" format to be copied in bucket\n",
        "$ aws s3 cp $(pwd) s3://mysecondbucket/ --recursive --exclude \"*\" --include \"l_*.json\"\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVWgvcily8b_"
      },
      "source": [
        "### Great, now that we have our files stored in buckets we can use a python script that will read the specified files in both buckets and compare the total count of grammar mistakes to word count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7mX201aq-CR"
      },
      "source": [
        "Open a text file and paste in the following python code. Title this script `grammarmistake.py`. This script can also be found on Github here.\n",
        "\n",
        "Keep in mind that you will have to change line number 18 and 37 to specify your unique bucket name and JSON file name.\n",
        "\n",
        "```\n",
        "#!/bin/bash\n",
        "\n",
        "import language_tool_python\n",
        "import boto3\n",
        "import json\n",
        "\n",
        "# mention the language keyword\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "\n",
        "# set counts of variables to 0\n",
        "num_mistakes_native = 0\n",
        "word_count_native = 0\n",
        "num_mistakes_learner = 0\n",
        "word_count_learner = 0\n",
        "\n",
        "# pulling files from s3 bucket for native speaker\n",
        "s3 = boto3.resource('s3')\n",
        "content_object = s3.Object('trans-native', 't_n_ital1.json')\n",
        "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
        "json_content = json.loads(file_content)\n",
        "\n",
        "# reading .json as string\n",
        "text = json_content['TranslatedText']\n",
        "\n",
        "# for loop for word count\n",
        "for i in range(len(text)):\n",
        "    if(text[i] == ' ' or text == '\\n' or text == '\\t'):\n",
        "        word_count_native = word_count_native + 1\n",
        "        \n",
        "# for loop for checking how many grammar mistakes\n",
        "for i in range(len(text)):\n",
        "    matches = tool.check(text[i])\n",
        "    num_mistakes_native = num_mistakes_native + len(matches)\n",
        "    \n",
        "# repeat process for the non-native speaker    \n",
        "# pulling files from s3 bucket for non-native \"learner\" speaker\n",
        "content_object = s3.Object('trans-learner', 't_l_ital1.json')\n",
        "file_content = content_object.get()['Body'].read().decode('utf-8')\n",
        "json_content = json.loads(file_content)\n",
        "\n",
        "# reading .json as string\n",
        "text = json_content['TranslatedText']\n",
        "\n",
        "# for loop for word count\n",
        "for i in range(len(text)):\n",
        "    if(text[i] == ' ' or text == '\\n' or text == '\\t'):\n",
        "        word_count_learner = word_count_learner + 1\n",
        "        \n",
        "# for loop for checking how many grammar mistakes\n",
        "for i in range(len(text)):\n",
        "    matches = tool.check(text[i])\n",
        "    num_mistakes_learner = num_mistakes_learner + len(matches)\n",
        "\n",
        "\n",
        "print(\"The number of words in the native speaker document is\", word_count_native)\n",
        "print(\"The number of mistakes in the native speaker document is\", num_mistakes_native)\n",
        "print(\"The number of words in the non-native speaker document is\", word_count_learner)\n",
        "print(\"The number of mistakes in the non-native speaker document is\", num_mistakes_learner)\n",
        "\n",
        "print(\"For the native speaker the grammar mistake rate is\", num_mistakes_native*100/word_count_native, \"%\")\n",
        "print(\"For the non-native speaker the grammar mistake rate is\", num_mistakes_learner*100/word_count_learner, \"%\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzGt6Jnzf-C"
      },
      "source": [
        "Now in the shell made the script executable by running the following command:\n",
        "```\n",
        "$ chmod u+x grammarmistake.py\n",
        "```\n",
        "And now lets execute the script. It may take a while to run depending on the length of the text.\n",
        "```\n",
        "$ python grammarmistake.py\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZdmkDpriSx-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtiCpD3DhEKf"
      },
      "source": [
        "### And here's the output using our data of a non-native Italian speaker and a native Italian speaker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5Qbe-sYg1mZ"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        ">>> The number of words in the native speaker document is: 145\n",
        ">>> The number of mistakes in native speaker document is: 13\n",
        ">>> The number of words in the non-native speaker document is: 140\n",
        ">>> The number of mistakes in the non-native speaker document is: 20\n",
        ">>> For the native speaker the grammar mistake rate is: 8.96551724137931 %\n",
        ">>> For the non-native speaker the grammar mistake rate is: 14.285714285714286 %\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd_9yqvuhXat"
      },
      "source": [
        "We see that the mistake rate is higher for the non-native speaker, which is not surprising, however we had hypothesized otherwise. Since this is only a very small sample size, no real conclusions can be drawn from this singular comparison. However, we can employ another machine learning service, Amazon Comprehend, to get a closer look at the readability of these texts. This can be tested for many different samples (though I struggled to make a for loop that could iterate over files in a bucket, hence the need to manually adjust the script). "
      ]
    }
  ]
}